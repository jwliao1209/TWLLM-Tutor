name: twllm_qlora_it

data:
  train_data_path: data/train_data/train_QB_history_9000.json
  valid_data_path: data/train_data/valid_QB_history_205.json
  with_answer_details: True

model:
  base_model_path: model_weight/Taiwan-LLM-7B-v2.0-chat
  lora_rank: 16
  lora_alpha: 16
  lora_dropout: 0.1
  nbit: 4

device:
  id: 0

train:
  epoch: 10
  batch_size: 4
  accum_grad_step: 4
  optimizer: adamw
  lr_scheduler: constant  # linear, constant, cosine, cosine_warmup
  lr: 0.0002
  weight_decay: 0.00001
  warm_up_step: 0
